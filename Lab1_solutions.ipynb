{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HQhanqiZHQ/DL_labs/blob/main/Lab1_solutions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGnMtz5kWezG"
      },
      "source": [
        "# Lab 1\n",
        "Today's lab consists of practice questions to review the topics presented thus far in class. We will be focusing on:\n",
        "1. Neural network terminology and architecture\n",
        "2. Python\n",
        "3. Forward and backward propagation\n",
        "\n",
        "### Question 1\n",
        "Let's review the terminology introduced by thinking about how to design a model for each the following scenarios. It's important to remember that while there is more than one correct answer in these cases, we want to develop an intuition to help save time in parameter tuning, training, computational resources, etc. We'll also briefy touch on some advanced topics to provide a foundation for later in the course, and remember you do not need to use a deep neural network in every case.\n",
        "\n",
        "*Case 1:* The input is the MNIST handwritten digits dataset (features are 28x28 pixel intensities and labels are digits 0-9). We want to predict which digit the image represents and there are only 10 images per category ($N=100$).\n",
        "\n",
        "    - Random forest, k-nearest neighbors because of the small sample size and relatively easy prediction task.\n",
        "\n",
        "*Case 2:* The identical setup but this time there are thousands of images per category.\n",
        "\n",
        "    - Either of the above methods are fine, or using a simple neural network. Activation function needs to be softmax and loss function needs to be categorical cross-entropy.\n",
        "\n",
        "*Case 3:* The identical setup as case 2 but this time images may contain multiple digits or no digits at all.\n",
        "\n",
        "    - Last layer activation should be sigmoid and BCE (binary cross_entropy).\n",
        "\n",
        "*Case 4:*  The covariates are BMI measurements and reported smoking status, the labels are binary denoting cardiovascular disease. Our sample consists of 70 individuals and we want to predict an individuals' health status based on their BMI and smoking status. We are interested in the effect of BMI on cardiovascular disease.\n",
        "\n",
        "    - Logistic regression.\n",
        "\n",
        "*Case 5:* The input consists of thousands of images of different animals and we want to classify which animal the image contains.\n",
        "\n",
        "    - CNN with softmax and CCE (categorical cross-entropy) or sigmoid and BCE (binary cross_entropy).\n",
        "\n",
        "*Case 6:* The input consists of thousands of English sentences and we want to predict the next word in the sentences.\n",
        "\n",
        "    - RNN\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdKVc4HJXHra"
      },
      "source": [
        "### Question 2\n",
        "\n",
        "Draw the architecture of a neural network satisying the following conditions:\n",
        "\n",
        "1. X is a univariate covariate. We will consider the case when X=5.\n",
        "2. There are two hidden layers. The first consists of two nodes, each with a bias term taking values (-1 and 1, respectively). The weight going to the first node takes value 0.5 and the weight going to the second node takes value -0.5.\n",
        "3. The nodes in hidden layer 1 each use a linear activation function.\n",
        "4. Hidden layer 2 consists of a single node with no bias term and the ReLU activation function. The weight from node 1 in hidden layer 1 is 0.3 and the weight from node 2 in hidden layer 1 is 0.7.\n",
        "5. Hidden layer 2 outputs to a single output node. The bias term for the output node is 0.5 and the weight from hidden layer 2 is 2.\n",
        "6. The loss function to be optimized is squared loss."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yol6nV5cuAVh"
      },
      "source": [
        "![network](https://drive.google.com/uc?id=12tva0pDMq4gCM_GABnbsmyh-TOL9lnU5)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqhSrFWpZNWr"
      },
      "source": [
        "### Question 3\n",
        "Implement a single forward pass of the network described in Question 3. You do not need to implement the network in keras and should instead use numpy operations (either scalar or matrix). Start by defining the weights and input matrices."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blT8waJAWbhj"
      },
      "source": [
        "# Import necessary packages\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7qaShKYZbAb"
      },
      "source": [
        "x = np.array([1, 5])                        # add bias/intercept as first entry\n",
        "w_hidden1 = np.matrix([[-1, 1], [.5, -.5]]) # 2x2 matrix of first-layer biases and weights\n",
        "w_hidden2 = np.matrix([[.3], [.7]])         # 1x2 matrix of second-layer weights\n",
        "w_out = 2                                   # 1x1 scalar of third-layer weights\n",
        "b_out = 0.5                                 # 1x1 scalar of third-layer bias"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMRFFW49Zqea"
      },
      "source": [
        "Now perform the forward pass."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNU-liiGYCjt"
      },
      "source": [
        "hidden1 = np.matmul(x,w_hidden1)         # perform matrix multiplication to get hidden layer 1\n",
        "hidden2 = np.matmul(hidden1,w_hidden2)   # perform matrix multiplication to get hidden layer 2\n",
        "hidden2_clamped = np.maximum(hidden2, 0) # relu\n",
        "y_hat = hidden2_clamped*w_out + b_out    # perform third multiplication to get output layer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjccTRMqZxx5"
      },
      "source": [
        "And let's print the values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBbrhmMxZ00o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "763e3db9-798b-470a-caa7-00cb663e5d4a"
      },
      "source": [
        "print('The values for the hidden layer 1 are:', hidden1)\n",
        "print('The values for the hidden layer 2 are:', hidden2)\n",
        "print('The post-relu values for the hidden layer 2 are:', hidden2_clamped)\n",
        "print('The value for the output layer is:', y_hat)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The values for the hidden layer 1 are: [[ 1.5 -1.5]]\n",
            "The values for the hidden layer 2 are: [[-0.6]]\n",
            "The post-relu values for the hidden layer 2 are: [[0.]]\n",
            "The value for the output layer is: [[0.5]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvci-2MiZ3Vg"
      },
      "source": [
        "Calculate the loss for the training example given a label of Y = 0.25."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dtmec9jLZ8Wi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "accf51ff-ba07-405f-da17-e4dbe769459c"
      },
      "source": [
        "y_i = 0.25 # positive outcome as defined in the problem\n",
        "loss_i = (y_i-y_hat)**2\n",
        "print('The loss is:',loss_i)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The loss is: [[0.0625]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJ8qWoFFaDpF"
      },
      "source": [
        "Implement a single backward pass of the network. Again use numpy. Start by defining the individual gradient terms."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZCP7P1JxaEIP"
      },
      "source": [
        "# gradient for loss\n",
        "dl_dy = 2*(y_i-y_hat) # gradient of loss wrt predicted probability (1x1)\n",
        "\n",
        "# gradients for output layer\n",
        "dy_dhidden2_clamped = w_out # gradient of y_hat wrt hidden output (1x1)\n",
        "dy_dw_out = hidden2_clamped # gradient of  y_hat wrt output layer weight (1x1)\n",
        "dy_db_out = 1 # gradient of  y_hat wrt output layer bias (1x1)\n",
        "\n",
        "# gradient (gate) for relu\n",
        "dhidden2_clamped_dhidden2 = (hidden2>0)*1 # gradient for relu\n",
        "\n",
        "# gradients for second hidden layer\n",
        "dhidden2_dw_2 = hidden1 # gradient of second hidden layer wrt second hidden layer weights\n",
        "dhidden2_dhidden1 = w_hidden2 # gradient of second hidden layer wrt first hidden layer output\n",
        "\n",
        "# gradients for first hidden layer\n",
        "dhidden1_dw_1 = x # gradient of first hidden layer wrt first hidden layer weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YfrUZMLJaHSL"
      },
      "source": [
        "dl_dw_out = dl_dy*dy_dw_out # gradient of loss wrt output weights\n",
        "dl_db_out = dl_dy*1         # gradient of loss wrt output bias\n",
        "dl_dw_2 = dl_dy*dy_dhidden2_clamped*dhidden2_clamped_dhidden2*dhidden2_dw_2                       # gradient of loss wrt second hidden layer weights\n",
        "dl_dw_11 = dl_dy*dy_dhidden2_clamped*dhidden2_clamped_dhidden2*dhidden2_dhidden1[0]*dhidden1_dw_1 # gradient of loss wrt first hidden layer weights (node 1)\n",
        "dl_dw_12 = dl_dy*dy_dhidden2_clamped*dhidden2_clamped_dhidden2*dhidden2_dhidden1[1]*dhidden1_dw_1 # gradient of loss wrt first hidden layer weights (node 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FsIdzUbEaNNR"
      },
      "source": [
        "We see that a maximum of two quantities are needed for each layer:\n",
        "    \n",
        "1. the partial derivative w.r.t. the input\n",
        "2. the partial derivative w.r.t. the weight/parameter\n",
        "    \n",
        "What is the purpose of each of these quantities?\n",
        "\n",
        "    - The partial derivative w.r.t. the input is used in the backprop/chain-rule calculations.\n",
        "    - The partial derivative w.r.t. the parameter/weights is used to update the parameter values in the training loop via gradient descent."
      ]
    }
  ]
}