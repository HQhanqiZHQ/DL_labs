{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HQhanqiZHQ/DL_labs/blob/main/ProblemSet1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhDo_JZP20DZ"
      },
      "source": [
        "# Problem Set 1\n",
        "### Due Wednesday, April 3rd by 11:59pm EST\n",
        "**Your Harvard ID**: 81631239\n",
        "\n",
        "Note: when completed, please download as an IPython Notebook file and upload to Canvas.\n",
        "\n",
        "### Question 1\n",
        "You will be performing one iteration of the forward pass and backpropagation calculations for a small network using Python. Here we will focus on the calculations for one training example, though in reality your data sets will be much larger and require matrix computation. You will also calculate the associated loss.\n",
        "\n",
        "Let $X_1 = 2$ and $X_2 = -1$ be the feature inputs and initialize the weights to be as shown in the figure below. This is a neural network with a single hidden layer consisting of three nodes. The blue numbers within each node represent the values for the bias terms and the black numbers along the edges represent the weights. The hidden layer outputs a single node, from which your task is binary classification. The label for this particular training example outcome is $y = 1$.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkhgwFtI3-6x"
      },
      "source": [
        "![network](https://drive.google.com/uc?id=1lfmGA56cIu81xD0y1SPKB7VuddS11G5o)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYD-V1epc8x4"
      },
      "source": [
        "Implement a single forward pass of the network. Assume the hidden layer uses a linear activation function (which is equivalent to assuming no activation function). You do not need to implement the network in keras and should instead use numpy operations (either scalar or matrix). Please use the variable names and print statements provided in the code chunks to display results for the TAs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDvRtn334Isx",
        "outputId": "dd4a103b-70bd-41e6-e1c2-80c34a04457e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Your code here\n",
        "import numpy as np\n",
        "# Initialize the inputs and weights\n",
        "X = np.array([2, -1])\n",
        "W_hidden = np.array([[1, 1.1], [0.2, 0], [-0.6, -0.3]])\n",
        "b_hidden = np.array([-1.8, -0.4, 0.96])\n",
        "W_output = np.array([0.5, 0.1, 1.3])\n",
        "b_output = 2\n",
        "\n",
        "# Perform the forward pass\n",
        "hidden = np.dot(W_hidden, X) + b_hidden\n",
        "output = np.dot(W_output, hidden) + b_output\n",
        "y_hat = 1 / (1 + np.exp(-output))  # Sigmoid for binary classification\n",
        "prediction = 1 if y_hat >= 0.5 else 0\n",
        "\n",
        "print('The values for the hidden layer are:', hidden)\n",
        "print('The value for the output layer is:', output)\n",
        "print('The predicted probability is:', y_hat)\n",
        "print('The prediction is:', prediction)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The values for the hidden layer are: [-0.9   0.    0.06]\n",
            "The value for the output layer is: 1.6280000000000001\n",
            "The predicted probability is: 0.8358954745887476\n",
            "The prediction is: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PqcsmDCc3_Qm"
      },
      "source": [
        "Calculate the loss for the training example making sure to select the appropriate loss function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nJ15bV_2yrk",
        "outputId": "e277bbd3-2910-407b-9f2d-6630f12b91eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Your code here\n",
        "# Given true label\n",
        "y_true = 1\n",
        "\n",
        "# Compute the loss using binary cross-entropy\n",
        "loss_i = - (y_true * np.log(y_hat) + (1 - y_true) * np.log(1 - y_hat))\n",
        "\n",
        "print('The loss is:',loss_i)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The loss is: 0.17925170411062194\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Z1mnvd6dPEC"
      },
      "source": [
        "Implement a single backward pass of the network. Again use numpy and report the values using the print statements provided. Please interpret these values. In other words, what are the values you just calculated used for?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxUbVHpQdSvP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97ce08a8-76ce-4147-f5fb-281f474ddf82"
      },
      "source": [
        "# Your code here\n",
        "# Forward pass was already computed; we are reusing the hidden, output, and y_hat variables.\n",
        "# Let's assume these variables are populated with the results from the forward pass we calculated earlier.\n",
        "\n",
        "# Backward pass: Compute the gradient of the loss w.r.t. the network's parameters\n",
        "d_loss_output = y_hat - y_true  # Gradient of loss w.r.t. output of network (pre-activation)\n",
        "d_loss_hidden = W_output * d_loss_output  # Gradient of loss w.r.t. hidden layer output\n",
        "\n",
        "# Gradient of loss w.r.t. hidden layer weights and biases\n",
        "d_loss_W_hidden = np.outer(d_loss_hidden, X)  # d_loss_hidden reshaped for outer product with X\n",
        "d_loss_b_hidden = d_loss_hidden  # Gradient of loss w.r.t. hidden layer bias is the same as d_loss_hidden\n",
        "\n",
        "# Gradient of loss w.r.t. output layer weights and bias\n",
        "d_loss_W_output = hidden * d_loss_output  # Element-wise multiplication\n",
        "d_loss_b_output = d_loss_output  # Gradient of loss w.r.t. output layer bias\n",
        "\n",
        "dl_dw_h = d_loss_W_output\n",
        "dl_db_h = d_loss_b_output\n",
        "\n",
        "dl_dw_1 = d_loss_W_hidden[0]\n",
        "dl_dw_2 = d_loss_W_hidden[1]\n",
        "dl_dw_3 = d_loss_W_hidden[2]\n",
        "\n",
        "print('The gradients of the loss wrt to the hidden weights are:', dl_dw_h)\n",
        "print('The gradient of the loss wrt to the hidden bias is:', dl_db_h)\n",
        "print('The gradients of the loss wrt to the input weights going to hidden node 1 are:', dl_dw_1)\n",
        "print('The gradients of the loss wrt to the input weights going to hidden node 2 are:', dl_dw_2)\n",
        "print('The gradients of the loss wrt to the input weights going to hidden node 3 are:', dl_dw_3)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The gradients of the loss wrt to the hidden weights are: [ 0.14769407 -0.         -0.00984627]\n",
            "The gradient of the loss wrt to the hidden bias is: -0.16410452541125242\n",
            "The gradients of the loss wrt to the input weights going to hidden node 1 are: [-0.16410453  0.08205226]\n",
            "The gradients of the loss wrt to the input weights going to hidden node 2 are: [-0.03282091  0.01641045]\n",
            "The gradients of the loss wrt to the input weights going to hidden node 3 are: [-0.42667177  0.21333588]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The gradients of the loss wrt to the hidden weights are: [ 0.14769407 -0.         -0.00984627] \\\n",
        "The gradient of the loss wrt to the hidden bias is: -0.16410452541125242 \\\n",
        "\n",
        "The gradients of the loss wrt to the input weights going to hidden node 1 are: [-0.16410453  0.08205226] \\\n",
        "The gradients of the loss wrt to the input weights going to hidden node 2 are: [-0.03282091  0.01641045] \\\n",
        "The gradients of the loss wrt to the input weights going to hidden node 3 are: [-0.42667177  0.21333588] \\\n",
        "A negative gradient implies that in order to minimize the loss, we should increase the corresponding weight in the next iteration.\n",
        "A positive gradient implies that we should decrease the weight to minimize the loss.\\\n",
        "For hidden weights (associated with the gradient 0.14769407 -0.         -0.00984627), implies that in order to minimize the loss, we should increase, decrease, and decrease the corresponding weights between hidden layers and output layers. \\\n",
        "\n",
        "The gradient of the loss with respect to the hidden bias is negative(-0.16410452541125242), implying in order to minimize the loss, we should decrease the bias. \\\n",
        "\n",
        "For hidden node 1:\n",
        "\n",
        "The weight from the first input feature (associated with the gradient -0.16410453) should be increased.\n",
        "The weight from the second input feature (associated with the gradient 0.08205226) should be decreased. \\\n",
        "For hidden node 2:\n",
        "\n",
        "The weight from the first input feature (associated with the gradient -0.03282091) should be increased.\n",
        "The weight from the second input feature (associated with the gradient 0.01641045) should be decreased. \\\n",
        "For hidden node 3:\n",
        "\n",
        "The weight from the first input feature (associated with the gradient -0.42667177) should be increased.\n",
        "The weight from the second input feature (associated with the gradient 0.21333588) should be decreased.\n",
        "\n",
        "The magnitude of these gradients gives an indication of how \"sensitive\" or \"influential\" a particular weight is with respect to the loss:\n",
        "\n",
        "Large negative gradients suggest that increasing the weight could potentially lead to a significant decrease in the loss.\n",
        "Small negative gradients indicate a lesser degree of sensitivity, implying that changes to the weight might result in smaller decreases in the loss.\n",
        "Large positive gradients suggest that decreasing the weight could lead to a significant decrease in the loss.\n",
        "Small positive gradients indicate that the weight has less influence on reducing the loss when it is decreased.\n",
        "\n",
        "Gradient with respect to the hidden weights indicate how much to adjust the weights for the output node. \\\n",
        "Gradient with respect to the hidden bias indicate how much to adjust the bias for the output node. \\\n",
        "For the first hidden node bias (with a gradient of -0.08205226), because the gradient is negative, it suggests that increasing the bias will help to decrease the loss. This can be thought of as adjusting the threshold at which the neuron activates, allowing it to fire more easily. \\\n",
        "For the second hidden node bias (with a gradient of -0.01641045), this value is also negative, but it's smaller in magnitude than the first node's gradient. This suggests that while the bias should still be increased to minimize the loss, the impact of this particular bias on the loss is less compared to the first node. \\\n",
        "For the third hidden node bias (with a gradient of -0.21333588), this gradient is the most negative among the three. This indicates a relatively larger potential reduction in loss by increasing the bias for this node. It may suggest that this node has a more substantial role or is more sensitive in the prediction errors being produced by the current model.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TQeu0D834yVR"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yhq7yh2dddJ"
      },
      "source": [
        "### Question 2\n",
        "In class we were considering classification problems where the goal was to predict a single discrete label of an input data point. Another common type of machine learning problem is \"regression\", which consists of predicting a continuous value instead of a discrete label. For instance, predicting the temperature tomorrow, given meteorological data, or predicting the time that a software project will take to complete, given its specifications.\n",
        "\n",
        "You will be attempting to predict the median price of homes in a given Boston suburb in the mid-1970s, given a few data points about the suburb at the time, such as the crime rate, the local property tax rate, etc.\n",
        "\n",
        "The dataset you will be using has another interesting difference from our previous examples: it has very few data points, only 506 in total, split between 404 training samples and 102 test samples, and each \"feature\" in the input data (e.g. the crime rate is a feature) has a different scale. For instance, some values are proportions, which take a value between 0 and 1, others take values between 1 and 12, others between 0 and 100.\n",
        "\n",
        "The data consists 13 features. The 13 features in the input data are as follows:\n",
        "\n",
        "1. Per capita crime rate.\n",
        "2. Proportion of residential land zoned for lots over 25,000 square feet.\n",
        "3. Proportion of non-retail business acres per town.\n",
        "4. Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).\n",
        "5. Nitric oxides concentration (parts per 10 million).\n",
        "6. Average number of rooms per dwelling.\n",
        "7. Proportion of owner-occupied units built prior to 1940.\n",
        "8. Weighted distances to five Boston employment centres.\n",
        "9. Index of accessibility to radial highways.\n",
        "10. Full-value property-tax rate per $10,000.\n",
        "11. Pupil-teacher ratio by town.\n",
        "12. 1000(Bk - 0.63)^2 where Bk is the proportion of Black people by town.\n",
        "13. % lower SES status of the population.\n",
        "\n",
        "The targets (outcomes, y) are the median values of owner-occupied homes, in thousands of dollars. The prices are typically between 10,000 and 50,000 dollars. If that sounds cheap, remember this was the mid-1970s, and these prices are not inflation-adjusted."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvYAHFmJdguK"
      },
      "source": [
        "# Import needed packages\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "import pandas as pd\n",
        "from tensorflow.keras.datasets import reuters\n",
        "from keras.utils import to_categorical\n",
        "from tensorflow.keras import layers\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGIw3FrDdkvU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "440a713f-1098-4d10-bbd5-1ad3cad6514d"
      },
      "source": [
        "# Load the data\n",
        "from keras.datasets import boston_housing\n",
        "\n",
        "(train_data, train_targets), (test_data, test_targets) =  boston_housing.load_data()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/boston_housing.npz\n",
            "57026/57026 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMSqxOTMdssk"
      },
      "source": [
        "Print the dimensions of the training set, i.e. its shape"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhGIEEbhdqYN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5476095-d5bd-48ce-9ec2-6c03eaf63cb8"
      },
      "source": [
        "# Your code here\n",
        "print(train_data.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(404, 13)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0NyaW24dzaA"
      },
      "source": [
        "Print the dimensions of the test set, i.e. its shape"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cn0dcQmzdz2I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20e71d6d-6ccb-4c32-bcb8-093a71c94e53"
      },
      "source": [
        "# Your code here\n",
        "print(test_data.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(102, 13)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sE0BC6OAd1Im"
      },
      "source": [
        "It would be problematic to feed into a neural network values that all take wildly different ranges. The network might be able to automatically adapt to such heterogeneous data, but it would definitely make learning more difficult. A widespread best practice to deal with such data is to do feature-wise normalization: for each feature in the input data (a column in the input data matrix), we will subtract the mean of the feature and divide by the standard deviation, so that the feature will be centered around 0 and will have a unit standard deviation.\n",
        "\n",
        "Normalize the data. Be sure to normalize the test set with the training set mean and standard deviation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LaLYdnb6d45w"
      },
      "source": [
        "# Your code here\n",
        "mean = train_data.mean(axis=0)\n",
        "std = train_data.std(axis=0)\n",
        "train_data -= mean\n",
        "train_data /= std\n",
        "test_data -= mean\n",
        "test_data /= std"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmVCNvzJd93b"
      },
      "source": [
        "Fit a fully connected neural network with 2 hidden layers and an output layer. Include 64 hidden units in each hidden layer and an appropriate number of units in the output layer. You are free to choose the activation functions. Use the `rmsprop` optimization function, and choose an appropriate loss function and model performance measure. Referring to the table shown in lectures 2 and 3 may help with these choices. Run the network for 50 epochs and use a batch size of 10."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f737O4fVd-cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fde8751-ba95-4f1b-90d7-13e4a76e4b95"
      },
      "source": [
        "# Your code here\n",
        "def build_model():\n",
        "    model = keras.Sequential([\n",
        "        layers.Dense(64, activation='relu', input_shape=(train_data.shape[1],)),\n",
        "        layers.Dense(64, activation='relu'),\n",
        "        layers.Dense(1)  # Single output node for regression\n",
        "    ])\n",
        "    model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n",
        "    return model\n",
        "\n",
        "model = build_model()\n",
        "model.fit(train_data, train_targets, epochs=50, batch_size=10, verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "41/41 [==============================] - 1s 2ms/step - loss: 427.3323 - mae: 18.5177\n",
            "Epoch 2/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 161.4739 - mae: 10.2613\n",
            "Epoch 3/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 58.8808 - mae: 5.7144\n",
            "Epoch 4/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 33.8611 - mae: 4.1696\n",
            "Epoch 5/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 25.9387 - mae: 3.5879\n",
            "Epoch 6/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 21.9441 - mae: 3.2773\n",
            "Epoch 7/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 19.1014 - mae: 3.0551\n",
            "Epoch 8/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 16.8161 - mae: 2.8940\n",
            "Epoch 9/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 15.0817 - mae: 2.7531\n",
            "Epoch 10/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 14.3921 - mae: 2.6530\n",
            "Epoch 11/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 13.2089 - mae: 2.5673\n",
            "Epoch 12/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 12.4916 - mae: 2.4910\n",
            "Epoch 13/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 11.7811 - mae: 2.4683\n",
            "Epoch 14/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 11.5993 - mae: 2.4408\n",
            "Epoch 15/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 10.9432 - mae: 2.3566\n",
            "Epoch 16/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 10.8057 - mae: 2.3274\n",
            "Epoch 17/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 10.6216 - mae: 2.3120\n",
            "Epoch 18/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 10.2547 - mae: 2.2477\n",
            "Epoch 19/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 10.0858 - mae: 2.2446\n",
            "Epoch 20/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 9.5832 - mae: 2.1747\n",
            "Epoch 21/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 9.4611 - mae: 2.2016\n",
            "Epoch 22/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 9.5383 - mae: 2.1764\n",
            "Epoch 23/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 9.0656 - mae: 2.1317\n",
            "Epoch 24/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 8.8196 - mae: 2.1289\n",
            "Epoch 25/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 8.8714 - mae: 2.0921\n",
            "Epoch 26/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 8.8234 - mae: 2.1053\n",
            "Epoch 27/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 8.6810 - mae: 2.0869\n",
            "Epoch 28/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 8.4702 - mae: 2.0168\n",
            "Epoch 29/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 8.1761 - mae: 2.0395\n",
            "Epoch 30/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 8.2342 - mae: 2.0132\n",
            "Epoch 31/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 8.1034 - mae: 2.0300\n",
            "Epoch 32/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 8.1460 - mae: 1.9940\n",
            "Epoch 33/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 8.0043 - mae: 1.9650\n",
            "Epoch 34/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 7.9149 - mae: 1.9751\n",
            "Epoch 35/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 7.6684 - mae: 1.9460\n",
            "Epoch 36/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 7.5552 - mae: 1.9251\n",
            "Epoch 37/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 7.6785 - mae: 1.9741\n",
            "Epoch 38/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 7.5654 - mae: 1.9226\n",
            "Epoch 39/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 7.4458 - mae: 1.8975\n",
            "Epoch 40/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 7.1082 - mae: 1.8644\n",
            "Epoch 41/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 7.0325 - mae: 1.8999\n",
            "Epoch 42/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 7.3143 - mae: 1.8952\n",
            "Epoch 43/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 7.2389 - mae: 1.8690\n",
            "Epoch 44/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 7.2717 - mae: 1.8750\n",
            "Epoch 45/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 6.9650 - mae: 1.8232\n",
            "Epoch 46/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 6.9529 - mae: 1.8654\n",
            "Epoch 47/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 6.8187 - mae: 1.8252\n",
            "Epoch 48/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 6.7254 - mae: 1.8199\n",
            "Epoch 49/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 6.7224 - mae: 1.8151\n",
            "Epoch 50/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 6.7307 - mae: 1.8451\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7e0c78624d00>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4vCuvMgeEt-"
      },
      "source": [
        "Report the test set accuracy (as in, a measure of how good your predictions are) and compare it to the training set accuracy. **Interpret what this means in words, in terms of what you are trying to do with your network**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6L5KOegeFSA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94d3fbd7-091c-43da-d911-e1655a39c2f6"
      },
      "source": [
        "# Your code here\n",
        "test_mse_score, test_mae_score = model.evaluate(test_data, test_targets)\n",
        "print(f\"Test MSE with 64 hidden units: {test_mse_score}\\nTest MAE with 64 hidden units: {test_mae_score}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 0s 4ms/step - loss: 18.5742 - mae: 2.6495\n",
            "Test MSE with 64 hidden units: 18.574249267578125\n",
            "Test MAE with 64 hidden units: 2.649498224258423\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAsjzPnmeFYW"
      },
      "source": [
        "Answer: Test MSE with 64 hidden units is 18.574249267578125.\n",
        "Test MAE with 64 hidden units is 2.649498224258423. An MSE of 18.5742 means that the average squared difference between the predicted and actual home prices (in thousands of dollars, squared) is around 18.5742. With an MAE of 2.6495, this means that on average, the model's predictions are off by about $2,649.50."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WiurzMl4eK_1"
      },
      "source": [
        "Now fit the same network as above but with 16 hidden nodes in each hidden layer. **What is the test set accuracy and how does it compare to the first network you created? Which model do you think is better?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jt993H3eLb9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e72fe455-8012-4bf5-bc09-d5e078eed136"
      },
      "source": [
        "# Your code here\n",
        "def build_model_with_16():\n",
        "    model = keras.Sequential([\n",
        "        layers.Dense(16, activation='relu', input_shape=(train_data.shape[1],)),\n",
        "        layers.Dense(16, activation='relu'),\n",
        "        layers.Dense(1)\n",
        "    ])\n",
        "    model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n",
        "    return model\n",
        "\n",
        "model_with_16 = build_model_with_16()\n",
        "model_with_16.fit(train_data, train_targets, epochs=50, batch_size=10, verbose=1)\n",
        "test_mse_score_16, test_mae_score_16 = model_with_16.evaluate(test_data, test_targets)\n",
        "print(f\"Test MSE with 16 hidden units: {test_mse_score_16}\\nTest MAE with 16 hidden units: {test_mae_score_16}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "41/41 [==============================] - 1s 2ms/step - loss: 554.5681 - mae: 21.8409\n",
            "Epoch 2/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 495.8397 - mae: 20.5566\n",
            "Epoch 3/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 423.4792 - mae: 18.8721\n",
            "Epoch 4/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 338.9770 - mae: 16.6196\n",
            "Epoch 5/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 246.7624 - mae: 13.7058\n",
            "Epoch 6/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 163.6617 - mae: 10.3775\n",
            "Epoch 7/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 103.1222 - mae: 7.7468\n",
            "Epoch 8/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 70.3271 - mae: 6.1411\n",
            "Epoch 9/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 53.8182 - mae: 5.1874\n",
            "Epoch 10/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 43.5869 - mae: 4.5190\n",
            "Epoch 11/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 36.6384 - mae: 4.0624\n",
            "Epoch 12/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 32.2886 - mae: 3.7601\n",
            "Epoch 13/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 29.5098 - mae: 3.6089\n",
            "Epoch 14/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 27.6606 - mae: 3.4538\n",
            "Epoch 15/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 25.6337 - mae: 3.3528\n",
            "Epoch 16/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 24.2923 - mae: 3.2869\n",
            "Epoch 17/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 23.2611 - mae: 3.1801\n",
            "Epoch 18/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 21.9219 - mae: 3.1319\n",
            "Epoch 19/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 20.8363 - mae: 3.0488\n",
            "Epoch 20/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 19.8297 - mae: 2.9667\n",
            "Epoch 21/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 18.9267 - mae: 2.9277\n",
            "Epoch 22/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 18.0410 - mae: 2.8948\n",
            "Epoch 23/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 17.6018 - mae: 2.8022\n",
            "Epoch 24/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 16.8779 - mae: 2.7715\n",
            "Epoch 25/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 16.2173 - mae: 2.7210\n",
            "Epoch 26/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 15.7850 - mae: 2.7028\n",
            "Epoch 27/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 15.2849 - mae: 2.6535\n",
            "Epoch 28/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 14.8564 - mae: 2.6464\n",
            "Epoch 29/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 14.4522 - mae: 2.5937\n",
            "Epoch 30/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 14.1309 - mae: 2.6009\n",
            "Epoch 31/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 13.6748 - mae: 2.5295\n",
            "Epoch 32/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 13.4938 - mae: 2.5477\n",
            "Epoch 33/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 13.2577 - mae: 2.5067\n",
            "Epoch 34/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 12.9590 - mae: 2.5004\n",
            "Epoch 35/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 12.6826 - mae: 2.4561\n",
            "Epoch 36/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 12.4452 - mae: 2.4261\n",
            "Epoch 37/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 12.3239 - mae: 2.4660\n",
            "Epoch 38/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 12.1337 - mae: 2.4155\n",
            "Epoch 39/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 11.9782 - mae: 2.3909\n",
            "Epoch 40/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 11.7180 - mae: 2.4017\n",
            "Epoch 41/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 11.6159 - mae: 2.3688\n",
            "Epoch 42/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 11.4427 - mae: 2.3520\n",
            "Epoch 43/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 11.3358 - mae: 2.3906\n",
            "Epoch 44/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 11.2394 - mae: 2.3496\n",
            "Epoch 45/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 10.9502 - mae: 2.3172\n",
            "Epoch 46/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 10.9907 - mae: 2.3172\n",
            "Epoch 47/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 10.8925 - mae: 2.2831\n",
            "Epoch 48/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 10.5408 - mae: 2.3014\n",
            "Epoch 49/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 10.6391 - mae: 2.2647\n",
            "Epoch 50/50\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 10.5222 - mae: 2.2615\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 25.3576 - mae: 3.2209\n",
            "Test MSE with 16 hidden units: 25.357641220092773\n",
            "Test MAE with 16 hidden units: 3.220874786376953\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvogMSN_eNPF"
      },
      "source": [
        "Answer: Test MSE with 16 hidden units is 25.357641220092773, larger than 18.574249267578125 which is the Test MSE with 64 hidden units. Test MAE with 16 hidden units is 3.220874786376953, larger than 2.649498224258423 which is the Test MAE with 64 hidden units. These suggest that the model with 64 hidden units is a better fit for this particular dataset since lower values of MSE and MAE indicate a model that is making predictions closer to the true values, which is preferred. However, this conclusion is drawn mostly based on the test results, and in real situations, sometimes a model with fewer parameters (like the one with 16 hidden units per layer) may generalize better and be less prone to overfitting, especially if the dataset is small."
      ]
    }
  ]
}